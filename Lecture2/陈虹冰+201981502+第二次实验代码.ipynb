{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66561af",
   "metadata": {},
   "source": [
    "# 简单的朴素贝叶斯分类样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd80354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0bcf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个单词列表\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 1是侮辱类 0为非侮辱类\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0427d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个不重复的列表包含全部词语\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  # 创建一个空集合\n",
    "    for document in dataSet:  # 将数据中的每个词添加进集合中 求并集\n",
    "        vocabSet = (vocabSet | set(document))\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa75b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入的单词列表与词典比对转化为01向量\n",
    "def words2Vec(vocabList, inputSet):  \n",
    "    returnVec = [0] * len(vocabList) # 创建空向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # 如输入单词存在于字典中则将对应位置赋值为1\n",
    "            returnVec[vocabList.index(word)] = 1 \n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5168f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainClass):\n",
    "    # trainMatrix嵌套数组的个数 即为文本数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])  # 文字个数\n",
    "    # 攻击性文本数/总文本数(先验概率)\n",
    "    pAbusive = sum(trainClass) / float(numTrainDocs)\n",
    "    # 初始化np数组 使用Laplace Smoothing避免出现0概率\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 类别为侮辱类\n",
    "        if trainClass[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        # 类别为非侮辱类\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 分别计算两类中该词汇出现的概率 取Log为了避免概率连乘导致过小\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53bd1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算概率进行分类\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # 分别计算该句话为侮辱性/非侮辱性的Log概率\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    # 比较两类概率哪个更大一些\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466c5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    # 读入词典\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    # 创建记录是否含有单词的训练向量\n",
    "    for post in listOPosts:\n",
    "        trainMat.append(words2Vec(myVocabList, post))\n",
    "    # 使用trainNB0函数计算概率\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    # 创建一个样例\n",
    "    testEntry = [['love', 'my', 'dalmation'], ['stupid', 'garbage'], \n",
    "                ['Think', 'He', 'Knows']]\n",
    "    # 将句子转化为向量并进行分类\n",
    "    for entry in testEntry:\n",
    "        thisDoc = np.array(words2Vec(myVocabList, entry))\n",
    "        print(entry, 'classified as: ', \n",
    "              classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a41238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n",
      "the word: Think is not in my Vocabulary!\n",
      "the word: He is not in my Vocabulary!\n",
      "the word: Knows is not in my Vocabulary!\n",
      "['Think', 'He', 'Knows'] classified as:  0\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b2deb",
   "metadata": {},
   "source": [
    "# 过滤垃圾邮件的简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7579dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子分割成单词并统一为小写\n",
    "def textParse(string): \n",
    "    # 使用正则表达式\\W+ 匹配非字母数字下划线字符1次以上\n",
    "    listOfTokens = re.split(r'\\W+', string)\n",
    "    res = []\n",
    "    for tok in listOfTokens:\n",
    "        if tok != '':\n",
    "            res.append(tok.lower())\n",
    "    # 如果该行为空则返回 None\n",
    "    return res if len(res) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5a2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    # 初始化一些数组\n",
    "    docList = []\n",
    "    classList = []\n",
    "    \n",
    "    # 分别读入0/1两类email\n",
    "    for i in range(1, 26):\n",
    "        with open('email/spam/%d.txt' % i) as spamFile:\n",
    "            for line in spamFile:\n",
    "                # 对每一行的文字进行分割\n",
    "                wordList = textParse(line)\n",
    "                # 如果wordList非空则进行以下操作\n",
    "                if wordList is not None:\n",
    "                    docList.append(wordList)\n",
    "                    classList.append(1)\n",
    "        with open('email/ham/%d.txt' % i) as hamFile:\n",
    "            for line in hamFile:\n",
    "                wordList = textParse(line)\n",
    "                if wordList is not None:\n",
    "                    docList.append(wordList)\n",
    "                    classList.append(0)\n",
    "    # 通过样例创建词典\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    # 随机抽取样本对NB分类器进行测试\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del (trainingSet[randIndex])\n",
    "    # 初始化训练矩阵\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        # 使用words2Vec函数将句子转化为向量\n",
    "        trainMat.append(words2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # 使用trainNB0函数计算先验概率\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    # 对测试集中的样本进行分类\n",
    "    for docIndex in testSet:\n",
    "        wordVector = words2Vec(vocabList, docList[docIndex])\n",
    "        # 使用classifyNB函数进行分类\n",
    "        res = classifyNB(np.array(wordVector), p0V, p1V, pSpam)\n",
    "        if res != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            # 打印错误分类的样本\n",
    "            print('ground truth:', classList[docIndex],\n",
    "                  \"| wrongly classified #\", docIndex, docList[docIndex], 'as', res)\n",
    "    # 打印随机抽取的10个样本中错误率\n",
    "    print('Error Rate: {} Error Number: {}'.format(float(errorCount/len(testSet)), errorCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a1e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth: 0 | wrongly classified # 3 ['hi', 'peter'] as 1\n",
      "Error Rate: 0.1 Error Number: 1\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85513d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
